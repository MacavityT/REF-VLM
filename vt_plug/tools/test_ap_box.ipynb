{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-07 20:31:45,244] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from PIL import Image\n",
    "from vt_plug.dataset.utils import de_norm_box_xyxy_square2origin, bbox_to_wh_coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box_path = \"/data/Aaronzhu/DatasetStage1/MSCOCO/2017/annotations/instances_val2017.json\"\n",
    "pred_save_path = \"/code/vt_plug/work_dirs/ablation/0305_det_no_match/eval_full/coco_det_class_small.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file = []\n",
    "with open(pred_save_path,'r') as f:\n",
    "    for line in f:\n",
    "        json_file = json.loads(line.strip())\n",
    "        all_file.extend(json_file)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36211/36211 [00:16<00:00, 2151.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_file_processed = []\n",
    "coco_val_image_dir = \"/data/Aaronzhu/DatasetStage1/MSCOCO/2017/val2017\"\n",
    "\n",
    "for file in tqdm(all_file):\n",
    "    image_id = str(file['image_id']).zfill(12)\n",
    "    image = Image.open(os.path.join(coco_val_image_dir,f\"{image_id}.jpg\"))\n",
    "    bbox = de_norm_box_xyxy_square2origin(file['bbox'],image.width,image.height)\n",
    "    bbox = bbox_to_wh_coco(bbox)\n",
    "    all_file_processed.append(\n",
    "        {'image_id':file['image_id'],\n",
    "         'category_id':file['category_id'],\n",
    "         'bbox':bbox,\n",
    "         'score':file['score']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_items = defaultdict(list)\n",
    "for i,item in enumerate(all_file_processed):\n",
    "    image_id = str(item['image_id']).zfill(12)\n",
    "    item['index'] = i\n",
    "    all_items[image_id].append(item)\n",
    "\n",
    "\n",
    "with open(gt_box_path,'r') as f:\n",
    "    gt_box = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "all_gt_items = defaultdict(list)\n",
    "for i,item in enumerate(gt_box['annotations']):\n",
    "    image_id = str(item['image_id']).zfill(12)\n",
    "    item['index'] = i\n",
    "    all_gt_items[image_id].append(item)\n",
    "\n",
    "\n",
    "all_images_items = defaultdict(list)\n",
    "for i,item in enumerate(gt_box['images']):\n",
    "    image_id = str(item['id']).zfill(12)\n",
    "    item['index'] = i\n",
    "    all_images_items[image_id].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "36781\n",
      "4858\n",
      "4858\n",
      "4858\n",
      "5000\n",
      "35985\n"
     ]
    }
   ],
   "source": [
    "def retain_common_keys(dict_a, dict_b, dict_c):\n",
    "    # 找到两个字典的键的交集\n",
    "    common_keys = dict_a.keys() & dict_b.keys() & dict_c.keys()\n",
    "    removed_keys_a = dict_a.keys() - common_keys\n",
    "    removed_keys_b = dict_b.keys() - common_keys\n",
    "    removed_keys_c = dict_c.keys() - common_keys\n",
    "    \n",
    "    # 根据交集过滤两个字典\n",
    "    filtered_dict_a = {key: dict_a[key] for key in common_keys}\n",
    "    filtered_dict_b = {key: dict_b[key] for key in common_keys}\n",
    "    filtered_dict_c = {key: dict_c[key] for key in common_keys}\n",
    "    \n",
    "    return filtered_dict_a, filtered_dict_b, filtered_dict_c, removed_keys_a, removed_keys_b, removed_keys_c\n",
    "\n",
    "\n",
    "all_items, all_gt_items, all_images_items, removed_keys_a, removed_keys_b, removed_keys_c = retain_common_keys(all_items, all_gt_items, all_images_items)\n",
    "print(len(gt_box['images']))\n",
    "print(len(gt_box['annotations']))\n",
    "print(len(all_images_items.keys()))\n",
    "print(len(all_items.keys()))\n",
    "print(len(all_gt_items.keys()))\n",
    "\n",
    "gt_box['images'] = [item for item in gt_box['images'] if item['id'] not in list(removed_keys_c)]\n",
    "# new_images = []\n",
    "# for item in gt_mask['images']:\n",
    "#     if item['id'] not in list(removed_keys_b):\n",
    "#         new_images.append(item)\n",
    "# gt_mask['images'] = new_images\n",
    "\n",
    "\n",
    "new_annotations = []\n",
    "# for item in gt_mask['annotations']:\n",
    "#     if item['image_id'] not in list(removed_keys_b):\n",
    "#         new_annotations.append(item)\n",
    "# gt_mask['annotations'] = new_annotations\n",
    "for key in all_gt_items.keys():\n",
    "    annotation = all_gt_items[key]\n",
    "    new_annotations.extend(annotation)\n",
    "gt_box['annotations'] = new_annotations\n",
    "\n",
    "\n",
    "# gt_mask['annotations'] = [item for item in gt_mask['annotations'] if item['image_id'] not in list(removed_keys_b)]\n",
    "print(len(gt_box['images']))\n",
    "print(len(gt_box['annotations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有重复的 id\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_ids(data):\n",
    "    seen_ids = set()\n",
    "    duplicate_ids = set()\n",
    "\n",
    "    # 遍历列表，检查每个id是否重复\n",
    "    for item in data:\n",
    "        if item['id'] in seen_ids:\n",
    "            duplicate_ids.add(item['id'])\n",
    "        else:\n",
    "            seen_ids.add(item['id'])\n",
    "    \n",
    "    # 如果有重复的id，打印出来\n",
    "    if duplicate_ids:\n",
    "        print(f\"重复的 id: {duplicate_ids}\")\n",
    "    else:\n",
    "        print(\"没有重复的 id\")\n",
    "\n",
    "find_duplicate_ids(gt_box['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=8.95s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.66s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.079\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.062\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.067\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.067\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.168\n"
     ]
    }
   ],
   "source": [
    "tmp_save_dir = \"/code/vt_plug/tmp\"\n",
    "split = 'val'\n",
    "\n",
    "# all_file = all_file[:16955]\n",
    "\n",
    "with open(f\"{tmp_save_dir}/box_coco2017_{split}.json\",'w') as f:\n",
    "    json.dump(all_file_processed,f)\n",
    "    f.close()\n",
    "\n",
    "with open(f\"{tmp_save_dir}/box_gt_coco2017_{split}.json\",'w') as f2:\n",
    "    json.dump(gt_box,f2)\n",
    "    f2.close()\n",
    "\n",
    "pred_save_path = f\"{tmp_save_dir}/box_coco2017_{split}.json\"\n",
    "gt_mask_path = f\"{tmp_save_dir}/box_gt_coco2017_{split}.json\"\n",
    "\n",
    "coco_gt = COCO(gt_mask_path)\n",
    "coco_dt = coco_gt.loadRes(pred_save_path)  # load predictions\n",
    "# Initialize COCOEval and specify the metric you want to use\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")  # \"segm\" for segmentation  bbox for detection\n",
    "# Evaluate on a specific category\n",
    "# coco_eval.params.catIds = [1]  # your category ID\n",
    "# Evaluate\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vt-plug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
